{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5281201",
   "metadata": {},
   "source": [
    "# Model Evaluation\n",
    "\n",
    "Evaluating trained models using standard metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c4f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd50f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# NOTE: trained_models, X_test_scaled, y_test must be available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6071d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f'\\n{name} Accuracy:', acc)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b2f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    results[name] = evaluate_model(name, model, X_test_scaled, y_test)\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index', columns=['Accuracy'])\n",
    "results_df.sort_values(by='Accuracy', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
